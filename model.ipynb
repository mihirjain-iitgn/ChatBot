{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled9.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkXY5v2IKMM_",
        "outputId": "b71e6ac4-d108-425d-a590-c52bb8498978"
      },
      "source": [
        "import json\n",
        "import pickle\n",
        "import random\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
        "from tensorflow.keras.optimizers import SGD"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4opGDg7zKPH8"
      },
      "source": [
        "class ChatBot():\n",
        "  def __init__(self, path):\n",
        "    self.words = []\n",
        "    self.classes = []\n",
        "    self.documents = []\n",
        "    ignore_words = ['?', '!']\n",
        "    dataset = open(path).read()\n",
        "    intents = json.loads(dataset)\n",
        "    for intent in intents['intents']:\n",
        "      for pattern in intent['patterns']:\n",
        "        n_words = nltk.word_tokenize(pattern)\n",
        "        self.words.extend(n_words)\n",
        "        self.documents.append((n_words, intent['tag']))\n",
        "        if intent['tag'] not in self.classes:\n",
        "          self.classes.append(intent['tag'])\n",
        "    temp = []\n",
        "    for word in self.words:\n",
        "      if word != \"?\" or word != \".\":\n",
        "        m_word = lemmatizer.lemmatize(word.lower())\n",
        "        temp.append(m_word)\n",
        "    self.words = sorted(list(set(temp)))\n",
        "    self.classes = sorted(list(set(self.classes)))\n",
        "    \n",
        "\n",
        "  def create_model(self, path):\n",
        "    self.model = Sequential()\n",
        "    self.model.add(Dense(160, input_shape = (len(self.X[0]),), activation = \"relu\"))\n",
        "    self.model.add(Dropout(0.5))\n",
        "    self.model.add(Dense(80, activation = \"relu\"))\n",
        "    self.model.add(Dropout(0.5))\n",
        "    self.model.add(Dense(40, activation = \"relu\"))\n",
        "    self.model.add(Dropout(0.5))\n",
        "    self.model.add(Dense(len(self.y[0]), activation = \"softmax\"))\n",
        "    sgd = SGD(learning_rate = 0.02, decay = 1e-5, momentum = 0.8, nesterov = True)\n",
        "    self.model.compile(loss = 'categorical_crossentropy', optimizer = sgd, metrics = ['accuracy'])\n",
        "    weights = self.model.fit(np.array(self.X), np.array(self.y), epochs = 150, batch_size = 5, verbose = 1)\n",
        "    self.model.save(path, weights)\n",
        "    \n",
        "\n",
        "  def create_train_data(self):\n",
        "    data = []\n",
        "    zeros = [0] * len(self.classes)\n",
        "    for document in self.documents:\n",
        "      bool_vec = []\n",
        "      patterns = document[0]\n",
        "      patterns = [lemmatizer.lemmatize(pattern.lower()) for pattern in patterns]\n",
        "      for w in self.words:\n",
        "        if w in patterns:\n",
        "          bool_vec.append(1)\n",
        "        else:\n",
        "          bool_vec.append(0)\n",
        "      one_hot = zeros.copy()\n",
        "      one_hot[self.classes.index(document[1])] = 1\n",
        "      data.append([bool_vec, one_hot])\n",
        "    random.shuffle(data)\n",
        "    data = np.array(data)\n",
        "    self.X = list(data[:,0])\n",
        "    self.y = list(data[:,1])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cperuT6dKlm1"
      },
      "source": [
        "cb = ChatBot(\"./intents.json\")\n",
        "cb.create_train_data()\n",
        "cb.create_model(\"./model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}